[
  {
    "objectID": "xgboost.html",
    "href": "xgboost.html",
    "title": "XGBoost Predictive Models",
    "section": "",
    "text": "Code\nlibrary(data.table)   \n\noutcome_data &lt;- fread(\"predict_outcome.csv.gz\")\n\n\n\n\nCode\n# Load library and prepare data\n#library(randomForest)\n#rf_data &lt;- copy(outcome_data)[, my_id := NULL]\n#rf_data[, conversion := as.factor(conversion)]\n\n# Run RF and get OOB AUC\n#rf_model &lt;- randomForest(conversion ~ ., data = rf_data, ntree = 100, importance = TRUE)\n#pred_oob &lt;- predict(rf_model, type = \"prob\")[,2]\n#cat(\"\\nOOB AUC:\", as.numeric(performance(prediction(pred_oob, as.numeric(as.character(rf_data$conversion))), \"auc\")@y.values))\n\n# Filter variables based on positive MDA\n#mda_scores &lt;- importance(rf_model)[, \"MeanDecreaseAccuracy\"]\n#keep_vars &lt;- unique(c(names(mda_scores[mda_scores &gt; 0]), \"my_id\", \"conversion\"))\n#outcome_data &lt;- outcome_data[, .SD, .SDcols = keep_vars]\n\n#cat(\"\\nVariables kept:\", length(keep_vars), \"out of\", ncol(rf_data) + 1)\n\n\n\n\nCode\n# Load required libraries\nlibrary(xgboost)\nlibrary(ROCR)\nlibrary(parallel)\n\n# Prepare the XGBoost matrices\nxs &lt;- model.matrix(~ . - 1 - conversion - my_id, data = outcome_data)\ny &lt;- as.numeric(as.character(outcome_data$conversion))\n\n# Create parameter grid\ngrid &lt;- expand.grid(\n  eta = seq(0.001, 0.1, by = 0.007),\n  max_depth = seq(7, 7, by = 2),\n  min_child_weight = seq(1, 1, by = 1),        \n  subsample = seq(0.8, 0.8, by = 0.2),\n  colsample_bytree = seq(0.8, 0.8, by = 0.2),\n  lambda = seq(1, 1, by = 1),                \n  alpha = seq(0, 0, by = 1),                  \n  gamma = seq(0.1, 0.1, by = 0.1),               \n  nrounds = seq(100, 400, by = 200)\n)\n\n# Sample grid points\nconf_lev &lt;- .95\nnum_max &lt;- 5\nn &lt;- ceiling(log(1-conf_lev)/log(1-num_max/nrow(grid)))\nind &lt;- sample(nrow(grid), n, replace = FALSE)\nrgrid &lt;- grid[ind, ]\n\n# Set up parallel processing\nnc &lt;- detectCores() - 1\n\n# Validation phase\ncat(\"\\nPhase 1: Validation Phase\\n\")\n\n\n\nPhase 1: Validation Phase\n\n\nCode\nn_validations &lt;- 2\nvalidation_results &lt;- matrix(nrow = nrow(rgrid), ncol = n_validations)\nvalidation_j_stats &lt;- matrix(nrow = nrow(rgrid), ncol = n_validations)\n\nfor (j in 1:nrow(rgrid)) {\n # cat(\"\\nTesting parameter set\", j, \"of\", nrow(rgrid), \"\\n\")\n  #cat(\"eta =\", rgrid[j, \"eta\"], \", nrounds =\", rgrid[j, \"nrounds\"], \"\\n\")\n  \n  for (i in 1:n_validations) {\n    # Create validation split\n    idx &lt;- unique(sample(nrow(xs), nrow(xs), T))\n    train_x &lt;- xs[idx, ]\n    train_y &lt;- y[idx]\n    val_x &lt;- xs[-idx, ]\n    val_y &lt;- y[-idx]\n    \n    prm &lt;- list(\n      booster = \"gbtree\",\n      objective = \"binary:logistic\",\n      max_depth = rgrid[j, \"max_depth\"],\n      eta = rgrid[j, \"eta\"],\n      subsample = rgrid[j, \"subsample\"],\n      colsample_bytree = rgrid[j, \"colsample_bytree\"],\n      gamma = rgrid[j, \"gamma\"],\n      min_child_weight = rgrid[j, \"min_child_weight\"],\n      alpha = rgrid[j, \"alpha\"],\n      lambda = rgrid[j, \"lambda\"],\n      nthread = nc\n    )\n    \n    dm_train &lt;- xgb.DMatrix(data = train_x, label = train_y)\n    mdl &lt;- xgb.train(\n      params = prm,\n      data = dm_train,\n      nrounds = rgrid[j, \"nrounds\"],\n      verbose = FALSE\n    )\n    \n    # Get predictions and ROC metrics\n    p &lt;- predict(mdl, xgb.DMatrix(data = val_x))\n    pred &lt;- prediction(p, val_y)\n    \n    # Calculate AUC\n    validation_results[j, i] &lt;- performance(pred, \"auc\")@y.values[[1]]\n    \n    # Calculate Youden's J Statistic\n    roc &lt;- performance(pred, \"tpr\", \"fpr\")\n    tpr &lt;- unlist(roc@y.values)\n    fpr &lt;- unlist(roc@x.values)\n    j_stats &lt;- tpr - fpr\n    validation_j_stats[j, i] &lt;- max(j_stats)\n  }\n  \n  #cat(\"Mean AUC:\", mean(validation_results[j,]), \"\\n\")\n  #cat(\"SD AUC:\", sd(validation_results[j,]), \"\\n\")\n  #cat(\"Mean J statistic:\", mean(validation_j_stats[j,]), \"\\n\")\n}\n\n# Select best parameters based on validation AUC\nbest_params_idx &lt;- which.max(rowMeans(validation_results))\nbest_params &lt;- rgrid[best_params_idx,]\ncat(\"\\nBest parameters:\\n\")\n\n\n\nBest parameters:\n\n\nCode\nprint(best_params)\n\n\n    eta max_depth min_child_weight subsample colsample_bytree lambda alpha\n2 0.008         7                1       0.8              0.8      1     0\n  gamma nrounds\n2   0.1     100\n\n\nCode\n# Run 100 tests with best parameters\ncat(\"\\nRunning 100 test iterations with best parameters...\\n\")\n\n\n\nRunning 100 test iterations with best parameters...\n\n\nCode\ntest_aucs &lt;- numeric(100)\ntest_j_stats &lt;- numeric(100)\nconfusion_matrices &lt;- list()\nperformance_list &lt;- list()\nall_thresholds &lt;- numeric(100)\n\n# Train final model with best parameters\nbest_params_list &lt;- as.list(best_params[-which(names(best_params) == \"nrounds\")])\nbest_params_list$booster &lt;- \"gbtree\"\nbest_params_list$objective &lt;- \"binary:logistic\"\nbest_params_list$nthread &lt;- nc\n\nfor(i in 1:100) {\n  #cat(\"\\nIteration\", i, \"of 100\\n\")       \n  # Create test split\n  idx &lt;- unique(sample(nrow(xs), nrow(xs), T))\n  train_x &lt;- xs[idx, ]\n  train_y &lt;- y[idx]\n  test_x &lt;- xs[-idx, ]\n  test_y &lt;- y[-idx]\n  \n  # Train model\n  dm_train &lt;- xgb.DMatrix(data = train_x, label = train_y)\n  mdl &lt;- xgb.train(\n    params = best_params_list,\n    data = dm_train,\n    nrounds = best_params[[\"nrounds\"]],\n    verbose = FALSE\n  )\n  \n  # Get predictions\n  p &lt;- predict(mdl, xgb.DMatrix(data = test_x))\n  pred &lt;- prediction(p, test_y)\n  \n  # Calculate AUC\n  test_aucs[i] &lt;- performance(pred, \"auc\")@y.values[[1]]\n  \n  # Calculate ROC curve and store\n  perf &lt;- performance(pred, \"tpr\", \"fpr\")\n  performance_list[[i]] &lt;- perf\n  \n  # Find optimal threshold using Youden's J statistic\n  tpr &lt;- unlist(perf@y.values)\n  fpr &lt;- unlist(perf@x.values)\n  j_stats &lt;- tpr - fpr\n  best_j_idx &lt;- which.max(j_stats)\n  optimal_threshold &lt;- unlist(perf@alpha.values)[best_j_idx]\n  all_thresholds[i] &lt;- optimal_threshold\n  test_j_stats[i] &lt;- j_stats[best_j_idx]\n  \n  # Create confusion matrix with optimal threshold (1-0 order with TP in top left)\n  pred_class &lt;- ifelse(p &gt;= optimal_threshold, 1, 0)\n  cm &lt;- table(factor(test_y, levels=c(1,0)), \n             factor(pred_class, levels=c(1,0)))\n  colnames(cm) &lt;- c(\"Pred 1\", \"Pred 0\")\n  rownames(cm) &lt;- c(\"True 1\", \"True 0\")\n  confusion_matrices[[i]] &lt;- cm\n}\n\n# Calculate average confusion matrix\navg_cm &lt;- Reduce('+', confusion_matrices) / length(confusion_matrices)\n\n# Print final results\ncat(\"\\nTest Results over 100 iterations:\\n\")\n\n\n\nTest Results over 100 iterations:\n\n\nCode\ncat(\"Mean AUC:\", mean(test_aucs), \"\\n\")\n\n\nMean AUC: 0.6762738 \n\n\nCode\ncat(\"SD AUC:\", sd(test_aucs), \"\\n\")\n\n\nSD AUC: 0.01108078 \n\n\nCode\ncat(\"Mean Youden's J:\", mean(test_j_stats), \"\\n\")\n\n\nMean Youden's J: 0.2833812 \n\n\nCode\ncat(\"SD Youden's J:\", sd(test_j_stats), \"\\n\")\n\n\nSD Youden's J: 0.01921065 \n\n\nCode\ncat(\"Mean Optimal Threshold:\", mean(all_thresholds), \"\\n\")\n\n\nMean Optimal Threshold: 0.5110999 \n\n\nCode\ncat(\"95% CI for AUC:\", mean(test_aucs) - 1.96 * sd(test_aucs), \n    \"to\", mean(test_aucs) + 1.96 * sd(test_aucs), \"\\n\")\n\n\n95% CI for AUC: 0.6545555 to 0.6979921 \n\n\nCode\ncat(\"\\nAverage Confusion Matrix:\\n\")\n\n\n\nAverage Confusion Matrix:\n\n\nCode\nprint(avg_cm)\n\n\n        \n         Pred 1 Pred 0\n  True 1 513.94 256.56\n  True 0 271.26 435.78\n\n\nCode\n# Plot ROC curve with confidence intervals\nplot(0:100/100, 0:100/100, type=\"l\", lty=2, \n     xlab=\"False Positive Rate\", ylab=\"True Positive Rate\",\n     main=\"ROC Curve\")\n\n# Calculate average ROC curve with confidence intervals\nfpr_grid &lt;- seq(0, 1, length.out = 100)\ntpr_matrix &lt;- matrix(NA, nrow = length(performance_list), ncol = length(fpr_grid))\n\nfor(i in seq_along(performance_list)) {\n  curve_i &lt;- performance_list[[i]]\n  tpr_matrix[i,] &lt;- approx(curve_i@x.values[[1]], \n                          curve_i@y.values[[1]], \n                          xout = fpr_grid)$y\n}\n\nmean_tpr &lt;- colMeans(tpr_matrix, na.rm = TRUE)\nlines(fpr_grid, mean_tpr, col=\"red\", lwd=2)\n\n# Add optimal threshold point\nopt_point_idx &lt;- which.min(abs(fpr_grid - mean(all_thresholds)))\npoints(fpr_grid[opt_point_idx], mean_tpr[opt_point_idx], \n       col=\"blue\", pch=19, cex=1.5)\n\n# Add confidence interval\nci_lower &lt;- apply(tpr_matrix, 2, function(x) quantile(x, 0.025, na.rm=TRUE))\nci_upper &lt;- apply(tpr_matrix, 2, function(x) quantile(x, 0.975, na.rm=TRUE))\nlines(fpr_grid, ci_lower, col=\"gray\", lty=2)\nlines(fpr_grid, ci_upper, col=\"gray\", lty=2)\n\n# Add legend\nlegend(\"bottomright\", \n       legend=c(\"Random\", \"Average ROC\", \"95% CI\", \"Optimal Threshold\"),\n       col=c(\"black\", \"red\", \"gray\", \"blue\"), \n       lty=c(2,1,2,NA), \n       pch=c(NA,NA,NA,19),\n       lwd=c(1,2,1,NA))\n\n\n\n\n\n\n\nCode\n# Load required libraries\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(ROCR)\nlibrary(parallel)\n\n# Prepare the XGBoost matrices\nxs &lt;- model.matrix(~ . - 1 - conversion - my_id, data = outcome_data)\ny &lt;- as.numeric(as.character(outcome_data$conversion))\n\n# Create parameter grid - only tune eta and nrounds\ngrid &lt;- expand.grid(\n  eta = seq(0.001, 0.1, by = 0.03),\n  nrounds = seq(50, 500, by = 100)\n)\n\n# Sample grid points\nconf_lev &lt;- .95\nnum_max &lt;- 5\nn &lt;- ceiling(log(1-conf_lev)/log(1-num_max/nrow(grid)))\nind &lt;- sample(nrow(grid), n, replace = FALSE)\nrgrid &lt;- grid[ind, ]\n\n# Set up parallel processing\nnc &lt;- detectCores() - 1\n\n# Validation phase\ncat(\"\\nPhase 1: Validation Phase\\n\")\n\n\n\nPhase 1: Validation Phase\n\n\nCode\nn_validations &lt;- 5\nvalidation_results &lt;- matrix(nrow = nrow(rgrid), ncol = n_validations)\nvalidation_j_stats &lt;- matrix(nrow = nrow(rgrid), ncol = n_validations)\n\nfor (j in 1:nrow(rgrid)) {\n  #cat(\"\\nTesting parameter set\", j, \"of\", nrow(rgrid), \"\\n\")\n  #cat(\"eta =\", rgrid[j, \"eta\"], \", nrounds =\", rgrid[j, \"nrounds\"], \"\\n\")\n  \n  for (i in 1:n_validations) {\n    # Create validation split\n    idx &lt;- unique(sample(nrow(xs), nrow(xs), T))\n    train_x &lt;- xs[idx, ]\n    train_y &lt;- y[idx]\n    val_x &lt;- xs[-idx, ]\n    val_y &lt;- y[-idx]\n    \n    prm &lt;- list(\n      booster = \"gblinear\",\n      objective = \"binary:logistic\",\n      eta = rgrid[j, \"eta\"],\n      nthread = nc\n    )\n    \n    dm_train &lt;- xgb.DMatrix(data = train_x, label = train_y)\n    mdl &lt;- xgb.train(\n      params = prm,\n      data = dm_train,\n      nrounds = rgrid[j, \"nrounds\"],\n      verbose = FALSE\n    )\n    \n    # Get predictions and ROC metrics\n    p &lt;- predict(mdl, xgb.DMatrix(data = val_x))\n    pred &lt;- prediction(p, val_y)\n    \n    # Calculate AUC\n    validation_results[j, i] &lt;- performance(pred, \"auc\")@y.values[[1]]\n    \n    # Calculate Youden's J Statistic\n    roc &lt;- performance(pred, \"tpr\", \"fpr\")\n    tpr &lt;- unlist(roc@y.values)\n    fpr &lt;- unlist(roc@x.values)\n    j_stats &lt;- tpr - fpr\n    validation_j_stats[j, i] &lt;- max(j_stats)\n  }\n  \n  #cat(\"Mean AUC:\", mean(validation_results[j,]), \"\\n\")\n  #cat(\"SD AUC:\", sd(validation_results[j,]), \"\\n\")\n  #cat(\"Mean J statistic:\", mean(validation_j_stats[j,]), \"\\n\")\n}\n\n# Select best parameters based on validation AUC\nbest_params_idx &lt;- which.max(rowMeans(validation_results))\nbest_params &lt;- rgrid[best_params_idx,]\ncat(\"\\nBest parameters:\\n\")\n\n\n\nBest parameters:\n\n\nCode\nprint(best_params)\n\n\n     eta nrounds\n17 0.001     450\n\n\nCode\n# Run X tests with best parameters\ncat(\"\\nRunning X test iterations with best parameters...\\n\")\n\n\n\nRunning X test iterations with best parameters...\n\n\nCode\ntest_aucs &lt;- c()\ntest_j_stats &lt;- c()\nconfusion_matrices &lt;- list()\nperformance_list &lt;- list()\nall_thresholds &lt;- c()\n\n# Train final model with best parameters\nbest_params_list &lt;- as.list(best_params[-which(names(best_params) == \"nrounds\")])\nbest_params_list$booster &lt;- \"gblinear\"\nbest_params_list$objective &lt;- \"binary:logistic\"\nbest_params_list$nthread &lt;- nc\n\nfor(i in 1:25) {\n  #cat(\"\\nIteration\", i, \"of 100\\n\")       \n  # Create test split\n  idx &lt;- unique(sample(nrow(xs), nrow(xs), T))\n  train_x &lt;- xs[idx, ]\n  train_y &lt;- y[idx]\n  test_x &lt;- xs[-idx, ]\n  test_y &lt;- y[-idx]\n  \n  # Train model\n  dm_train &lt;- xgb.DMatrix(data = train_x, label = train_y)\n  mdl &lt;- xgb.train(\n    params = best_params_list,\n    data = dm_train,\n    nrounds = best_params[[\"nrounds\"]],\n    verbose = FALSE\n  )\n  \n  # Get predictions\n  p &lt;- predict(mdl, xgb.DMatrix(data = test_x))\n  pred &lt;- prediction(p, test_y)\n  \n  # Calculate AUC\n  test_aucs[i] &lt;- performance(pred, \"auc\")@y.values[[1]]\n  \n  # Calculate ROC curve and store\n  perf &lt;- performance(pred, \"tpr\", \"fpr\")\n  performance_list[[i]] &lt;- perf\n  \n  # Find optimal threshold using Youden's J statistic\n  tpr &lt;- unlist(perf@y.values)\n  fpr &lt;- unlist(perf@x.values)\n  j_stats &lt;- tpr - fpr\n  best_j_idx &lt;- which.max(j_stats)\n  optimal_threshold &lt;- unlist(perf@alpha.values)[best_j_idx]\n  all_thresholds[i] &lt;- optimal_threshold\n  test_j_stats[i] &lt;- j_stats[best_j_idx]\n  \n  # Create confusion matrix with optimal threshold (1-0 order with TP in top left)\n  pred_class &lt;- ifelse(p &gt;= optimal_threshold, 1, 0)\n  cm &lt;- table(factor(test_y, levels=c(1,0)), \n             factor(pred_class, levels=c(1,0)))\n  colnames(cm) &lt;- c(\"Pred 1\", \"Pred 0\")\n  rownames(cm) &lt;- c(\"True 1\", \"True 0\")\n  confusion_matrices[[i]] &lt;- cm\n}\n\n# Calculate average confusion matrix\navg_cm &lt;- Reduce('+', confusion_matrices) / length(confusion_matrices)\n\n# Print final results\ncat(\"\\nTest Results over 100 iterations:\\n\")\n\n\n\nTest Results over 100 iterations:\n\n\nCode\ncat(\"Mean AUC:\", mean(test_aucs), \"\\n\")\n\n\nMean AUC: 0.6490824 \n\n\nCode\ncat(\"SD AUC:\", sd(test_aucs), \"\\n\")\n\n\nSD AUC: 0.01376997 \n\n\nCode\ncat(\"Mean Youden's J:\", mean(test_j_stats), \"\\n\")\n\n\nMean Youden's J: 0.2374326 \n\n\nCode\ncat(\"SD Youden's J:\", sd(test_j_stats), \"\\n\")\n\n\nSD Youden's J: 0.02486402 \n\n\nCode\ncat(\"Mean Optimal Threshold:\", mean(all_thresholds), \"\\n\")\n\n\nMean Optimal Threshold: 0.5022285 \n\n\nCode\ncat(\"95% CI for AUC:\", mean(test_aucs) - 1.96 * sd(test_aucs), \n    \"to\", mean(test_aucs) + 1.96 * sd(test_aucs), \"\\n\")\n\n\n95% CI for AUC: 0.6220932 to 0.6760715 \n\n\nCode\ncat(\"\\nAverage Confusion Matrix:\\n\")\n\n\n\nAverage Confusion Matrix:\n\n\nCode\nprint(avg_cm)\n\n\n        \n         Pred 1 Pred 0\n  True 1 518.00 250.36\n  True 0 312.44 401.80\n\n\nCode\n# Plot ROC curve with confidence intervals\nplot(0:100/100, 0:100/100, type=\"l\", lty=2, \n     xlab=\"False Positive Rate\", ylab=\"True Positive Rate\",\n     main=\"ROC Curve\")\n\n# Calculate average ROC curve with confidence intervals\nfpr_grid &lt;- seq(0, 1, length.out = 100)\ntpr_matrix &lt;- matrix(NA, nrow = length(performance_list), ncol = length(fpr_grid))\n\nfor(i in seq_along(performance_list)) {\n  curve_i &lt;- performance_list[[i]]\n  tpr_matrix[i,] &lt;- approx(curve_i@x.values[[1]], \n                          curve_i@y.values[[1]], \n                          xout = fpr_grid)$y\n}\n\nmean_tpr &lt;- colMeans(tpr_matrix, na.rm = TRUE)\nlines(fpr_grid, mean_tpr, col=\"red\", lwd=2)\n\n# Add optimal threshold point\nopt_point_idx &lt;- which.min(abs(fpr_grid - mean(all_thresholds)))\npoints(fpr_grid[opt_point_idx], mean_tpr[opt_point_idx], \n       col=\"blue\", pch=19, cex=1.5)\n\n# Add confidence interval\nci_lower &lt;- apply(tpr_matrix, 2, function(x) quantile(x, 0.025, na.rm=TRUE))\nci_upper &lt;- apply(tpr_matrix, 2, function(x) quantile(x, 0.975, na.rm=TRUE))\nlines(fpr_grid, ci_lower, col=\"gray\", lty=2)\nlines(fpr_grid, ci_upper, col=\"gray\", lty=2)\n\n# Add legend\nlegend(\"bottomright\", \n       legend=c(\"Random\", \"Average ROC\", \"95% CI\", \"Optimal Threshold\"),\n       col=c(\"black\", \"red\", \"gray\", \"blue\"), \n       lty=c(2,1,2,NA), \n       pch=c(NA,NA,NA,19),\n       lwd=c(1,2,1,NA))"
  }
]